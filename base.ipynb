{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import efficientnet_builder\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "  path = './'\n",
    "  train = pd.read_csv(path+'CheXpert-v1.0-small/train.csv')\n",
    "  valid = pd.read_csv(path+'CheXpert-v1.0-small/valid.csv')\n",
    "  \n",
    "  train['validation'] = False\n",
    "  valid['validation'] = True\n",
    "  df = pd.concat([train, valid])\n",
    "  \n",
    "  columns = ['Path', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion', 'validation']\n",
    "  df = df[columns]\n",
    "  \n",
    "  for feature in ['Atelectasis', 'Edema']:\n",
    "      df[feature] = df[feature].apply(lambda x: 1 if x==-1 else x)\n",
    "  \n",
    "  for feature in ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']:\n",
    "      df[feature] = df[feature].apply(lambda x: 0 if x==-1 else x)\n",
    "  df.fillna(0, inplace=True)\n",
    "  \n",
    "  train = df[~df.validation][:50]\n",
    "  print(len(train))\n",
    "  train_files = train['Path'].tolist()\n",
    "  train_files = [path+fil for fil in train_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  train_labels = np.array(train[columns])\n",
    "  \n",
    "  valid = df[df.validation][:50]\n",
    "  print(len(valid))\n",
    "  valid_files = valid['Path'].tolist()\n",
    "  valid_files = [path+fil for fil in valid_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  valid_labels = np.array(valid[columns])  \n",
    "  return train_files, train_labels, valid_files, valid_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size 224\n"
     ]
    }
   ],
   "source": [
    "MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "\n",
    "model_name='efficientnet-b0'\n",
    "batch_size=32\n",
    "\"\"\"Initialize internal variables.\"\"\"\n",
    "model_name = model_name\n",
    "batch_size = batch_size\n",
    "num_classes = 1000\n",
    "# Model Scaling parameters\n",
    "_, _, image_size, _ = efficientnet_builder.efficientnet_params(\n",
    "      model_name)\n",
    "print('image_size', image_size)\n",
    "\n",
    "def restore_model(sess, ckpt_dir):\n",
    "  \"\"\"Restore variables from checkpoint dir.\"\"\"\n",
    "  checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n",
    "  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "  ema_vars = tf.trainable_variables() + tf.get_collection('moving_vars')\n",
    "  for v in tf.global_variables():\n",
    "    if 'moving_mean' in v.name or 'moving_variance' in v.name:\n",
    "      ema_vars.append(v)\n",
    "  ema_vars = list(set(ema_vars))\n",
    "  var_dict = ema.variables_to_restore(ema_vars)\n",
    "  saver = tf.train.Saver(var_dict, max_to_keep=1)\n",
    "  saver.restore(sess, checkpoint)\n",
    "  return saver\n",
    "def build_model(features, is_training):\n",
    "  \"\"\"Build model with input features.\"\"\"\n",
    "  features -= tf.constant(MEAN_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
    "  features /= tf.constant(STDDEV_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
    "  out, _ = efficientnet_builder.build_model_base(\n",
    "      features, model_name, is_training)\n",
    "  return out\n",
    "\n",
    "def build_dataset(filenames, labels, is_training):\n",
    "  \"\"\"Build input dataset.\"\"\"\n",
    "  filenames = tf.constant(filenames)\n",
    "  labels = tf.constant(labels)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  return dataset\n",
    "def _parse_function(filename, label):\n",
    "  image_string = tf.read_file(filename)\n",
    "  image_decoded = preprocessing.preprocess_image(\n",
    "        image_string, is_training, image_size=image_size)\n",
    "  image = tf.cast(image_decoded, tf.float32)\n",
    "  return image, label\n",
    "\n",
    "  dataset = dataset.map(_parse_function)\n",
    "  dataset = dataset.batch(batch_size)#.repeat()\n",
    "  return dataset\n",
    "\n",
    "def _loss(x, y):\n",
    "  logits = tf.contrib.layers.fully_connected(x, 5, activation_fn=None)  \n",
    "  predicts = tf.math.sigmoid(logits, name = 'sigmoid_logits')\n",
    "  cross_entropy = tf.losses.sigmoid_cross_entropy(logits=logits,\n",
    "                                                  multi_class_labels=y)\n",
    "  weight_decay = 1e-5\n",
    "  loss = cross_entropy + weight_decay * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n",
    "       if 'batch_normalization' not in v.name])\n",
    "  return loss, predicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected string, got 123.675 of type 'float' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d52d591fff07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mvalid_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtrain_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mtrain_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-08639cc90fa9>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(features, is_training)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;34m\"\"\"Build model with input features.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mfeatures\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEAN_RGB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTDDEV_RGB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   out, _ = efficientnet_builder.build_model_base(\n",
      "\u001b[0;32m~/base_env/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant_v1\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \"\"\"\n\u001b[1;32m    178\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=verify_shape,\n\u001b[0;32m--> 179\u001b[0;31m                         allow_broadcast=False)\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base_env/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/base_env/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    464\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base_env/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 371\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string, got 123.675 of type 'float' instead."
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  train_files, train_labels, validation_files, validation_labels = get_data()\n",
    "\n",
    "  train_dataset = build_dataset(train_files, train_labels, True)\n",
    "  valid_dataset = build_dataset(validation_files, validation_labels, False)\n",
    "  \n",
    "  train_iter = train_dataset.make_initializable_iterator()\n",
    "  valid_iter = valid_dataset.make_initializable_iterator()\n",
    "  \n",
    "  train_images, train_labels = train_iter.get_next()\n",
    "  valid_images, valid_labels = valid_iter.get_next()\n",
    "\n",
    "  train_out = build_model(train_images, is_training=True)\n",
    "  train_out = tf.reduce_mean(train_out, axis=[1,2])\n",
    "\n",
    "\n",
    "  with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    valid_out = build_model(valid_images, is_training=False)\n",
    "    valid_out = tf.reduce_mean(valid_out, axis=[1,2])\n",
    "  \n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  saver = restore_model(sess, './weights_efficientnet-b0/')\n",
    "\n",
    "  temp = set(tf.all_variables())\n",
    "\n",
    "  train_loss, _ = _loss(train_out, train_labels)\n",
    "  valid_loss, predicts = _loss(valid_out, valid_labels)\n",
    "  \n",
    "  lr = 0.0001\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=lr, name=\"adam\").minimize(train_loss)\n",
    "  \n",
    "  sess.run(tf.initialize_variables(set(tf.all_variables()) - temp))\n",
    "  epochs = 3\n",
    "  n_batches_train = len(train_files)//batch_size\n",
    "  n_batches_valid = len(valid_files)//batch_size\n",
    "  saver = tf.train.Saver()\n",
    "  print(n_batches_train, n_batches_valid)\n",
    "  for epoch in range(epochs):\n",
    "    # training\n",
    "    sess.run(train_iter.initializer)\n",
    "    _train_loss = 0\n",
    "    for i in range(n_batches_train):\n",
    "      loss_value, _ = sess.run([train_loss, optimizer])\n",
    "      if i>1 and i%1000==0:\n",
    "        print('iteration: {} | loss {}'.format(i, loss_value))\n",
    "      _train_loss += loss_value\n",
    "    _train_loss = _train_loss/n_batches_train\n",
    "    \n",
    "    # validation\n",
    "    sess.run(valid_iter.initializer)\n",
    "    _valid_loss = 0\n",
    "    predictions = []\n",
    "    for i in range(n_batches_valid+1):\n",
    "      loss_value, prediction = sess.run([valid_loss, predicts])\n",
    "      _valid_loss += loss_value\n",
    "      predictions.append(prediction)\n",
    "      print('prediction', prediction.shape)\n",
    "    _valid_loss = _valid_loss/n_batches_valid\n",
    "#     calculate_metric(predictions, validation_labels)\n",
    "    \n",
    "    print(\"epoch: {} | training loss: {} | validation loss: {}\".format(epoch+1, _train_loss, _valid_loss ))\n",
    "    saver.save(sess, \"./saved_model/model{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
